1. 什么是决策树

   ​	决策树表示基于特征对实例进行分类的树形结构，从给定的训练数据集中，递归选择最优划分特征，依据此特征对训练数据集进行划分，直到结点符合停止条件。决策树可以看作是一系列 `if-then` 规则的集合。

2. 停止条件

   - 当前结点所有样本属于同一类别。
   - 当前结点属性集为空，或者是所有样本在所有属性上取值相同。
   - 当前结点无样本。

3. 决策树分类

   - ID3

     - 信息熵：

     $$
     Ent(D)=-\sum_{k=1}^{|y|}p_k\log{p_k}
     $$
     - 信息增益：

     $$
     Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)
     $$
     其中，a为所选属性，V为所选属性的取值数量，$|D^v|$为子集$D^v$的样本数量。

     - ID3算法选择信息增益最大的属性进行划分。

     - 缺点：偏向于选择取值较多的属性，容易形成一棵浅而宽的树

   - C4.5

     - 信息增益比（信息增益率）：
       $$
       Gain_{ratio}(D,a)=\frac{Gain(D,a)}{H_a(D)}
       $$
       其中
       $$
       H_a(D)=-\sum_{v=1}^V\frac{|D^v|}{|D|}\log{\frac{|D^v|}{|D|}}
       $$
       为将属性a作为label时的经验熵。

     - 缺点：偏向于选择取值较少的属性，因此并不是直接选择信息增益率最大的特征，而是现在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征。

   - CART（分类树）

     - 基尼不纯度：
       $$
       Gini(D)=\sum_{y=1}^{|y|}p_k(1-p_k)=1-\sum_{y=1}^{|y|}p_k^2
       $$
       基尼不纯度为被错误分类的期望值，选择基尼不纯度最小的属性进行划分。

     - CART树为二叉树，每次对每个属性进行二元化分，对于离散属性只区分是不是等于该取值，对于连续属性则以是否大于该取值划分。

   - CART（回归树）
     - 输出y为连续变量，将输出划分为M个区域R<sub>1</sub>，R<sub>2</sub>,...,R<sub>M</sub>,各个区域的输出值分别为c<sub>1</sub>,c<sub>2</sub>,...,c<sub>M</sub>，求导易得当c<sub>i</sub>为各区域输出的平均值时误差最小。
     - 对每个属性的每个取值进行二元划分，取划分后误差最小的属性的取值进行划分。

4. 决策树为什么不需要归一化

   ​	因为数值缩放不影响分裂点位置，对树模型的结构不造成影响。 按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。而且，树模型是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化。

   ​	既然树形结构（如决策树、RF）不需要归一化，那为何非树形结构比如Adaboost、SVM、LR、Knn、KMeans之类则需要归一化。

   ​	对于线性模型，特征值差别很大时，运用梯度下降的时候，损失等高线是椭圆形，需要进行多次迭代才能到达最优点。 但是如果进行了归一化，那么等高线就是圆形的，促使SGD往原点迭代，从而导致需要的迭代次数较少。

5. 决策树的剪枝
   - 预剪枝：其中的核心思想就是，在每一次实际对结点进行进一步划分之前，先采用验证集的数据来验证如果划分是否能提高划分的准确性。如果不能，就把结点标记为叶结点并退出进一步划分；如果可以就继续递归生成节点。
   - 后剪枝：后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来泛化性能提升，则将该子树替换为叶结点。



