1. 最大熵原理

   最大熵原理是概率模型学习的一个准则，其认为学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型。

   通常用约束条件来确定概率模型的集合，然后在集合中选择熵最大的模型。

   直观地，最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件。在没有更多信息的情况下，那些不确定的部分都是等可能的。最大熵原理通过熵的最大化来表示等可能性，因为当X服从均匀分布时熵最大。

2. 最大熵模型

   最大熵原理应用到分类得到最大熵模型。

   给定训练集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$，联合分布P(X,Y)以及边缘分布P(X)的经验分布都可以由训练数据得到：
$$
   \widetilde{P}(X=x,Y=y)=\frac{count(X=x,Y=y)}{N}
   \\ \widetilde{P}(X=x)=\frac{count(X=x)}{N}
   $$
   用特征函数f(x,y)描述输入x和输出y之间的某一个事实，特征函数是一个二值函数，当x与y满足某一事实时取1，否则取0。例如，可以令特征x与标签y在训练集出现过时取1，否则取0。
   
   特征函数f(x,y)关于经验分布$\widetilde{P}(X=x,Y=y)$的期望值为：
   $$
   E_{\widetilde{P}}(f)=\sum_{x,y}\widetilde{P}(x,y)f(x,y)
   $$
   特征函数f(x,y)关于模型P(Y|X)与经验分布$\widetilde{P}(x)$的期望值为：
   $$
   E_{P}(f)=\sum_{x,y}\widetilde{P}(x)P(y|x)f(x,y)
   $$
   如果模型能够获取训练数据中的信息，那么就可以假设这两个期望值相等，即：
   $$
   \sum_{x,y}\widetilde{P}(x,y)f(x,y)=\sum_{x,y}\widetilde{P}(x)P(y|x)f(x,y)
   $$
   将上式作为模型学习的约束条件，条件数量对应特征函数个数，设所有满足约束条件的模型集合为：
   $$
   C=\{P|\sum_{x,y}\widetilde{P}(x,y)f_i(x,y)=\sum_{x,y}\widetilde{P}(x)P(y|x)f_i(x,y),\quad i=1,2,...,n\}
   $$
   其中n为特征函数个数。
   
   定义在条件概率分布P(Y|X)上的条件概率熵为：
   $$
   H(P)=-\sum_{x,y}\widetilde{P}(x)P(y|x)\ln{P(y|x)}
   $$
   模型集合C中条件熵H(P)最大的模型称为最大熵模型。

3. 最大熵模型的学习

   最大熵模型的学习过程就是求解最大熵模型的过程，等价于求解以下最优化问题：
   $$
   \max H(P)=-\sum_{x,y}\widetilde{P}(x)P(y|x)\ln{P(y|x)}
   \\ s.t. \qquad \sum_{x,y}\widetilde{P}(x,y)f_i(x,y)=\sum_{x,y}\widetilde{P}(x)P(y|x)f_i(x,y),\quad i=1,2,...,n
   \\ \sum_{x,y}P(y|x)\widetilde{P}(x)=1
   $$
   （上式中最后一个约束条件与《统计学习方法》中给出的不同，参考https://zhuanlan.zhihu.com/p/83765331，我认为书中给出的有误，而应该像上式这样才能得出最后的结果。）

   按照最优化问题的习惯，求解与上述问题等价的$\min -H(p)$。

   引入拉格朗日乘子$w_0,w_1,...,w_n$将上述带约束条件的最优化问题转化为无约束的最优化问题，定义拉格朗日函数：
   $$
   L(P,W)=-H(P)+w_0(1-\sum_{x,y}P(y|x)\widetilde{P}(x))+\sum_{i=1}^nw_i(\sum_{x,y}\widetilde{P}(x,y)f_i(x,y)-\sum_{x,y}\widetilde{P}(x)P(y|x)f_i(x,y))
   \\ = \sum_{x,y}\widetilde{P}(x)P(y|x)\ln{P(y|x)} +w_0(1-\sum_{x,y}P(y|x)\widetilde{P}(x))+\sum_{i=1}^nw_i(\sum_{x,y}\widetilde{P}(x,y)f_i(x,y)-\sum_{x,y}\widetilde{P}(x)P(y|x)f_i(x,y))
   $$
   类似于**SVM**一节，这里最优化的原始问题是：
   $$
   \min_{P\in{C}}\max_wL(P,w)
   $$
   对偶问题是：
   $$
   \max_w\min_{P\in{C}}L(P,w)
   $$
   在上式内部的最小化部分固定w，此时L(P,w)是关于P的函数，用$\theta(P)$表示，对P(y|x)求偏导数得：
   $$
   \frac{\partial{\theta(P)}}{\partial{P(y|x)}}=\widetilde{P}(x)(1+\ln{P(y|x)})-\widetilde{P}(x)w_0-\widetilde{P}(x)\sum_{i=1}^nw_if_i(x,y)
   \\ = \widetilde{P}(x)(1+\ln{P(y|x)}-w_0-\sum_{i=1}^nw_if_i(x,y))
   $$
   令偏导数等于0，因为$\widetilde{P}(x)>0$，所以$1+\ln{P(y|x)}-w_0-\sum_{i=1}^nw_if_i(x,y)=0$，解得：
   $$
   P(y|x)=\frac{e^{\sum_{i=1}^nw_if_i(x,y)}}{e^{1-w_0}}
   $$
   由于$\sum_yP(y|x)=1$，得到P(y|x)关于w的表达式为：
   $$
   P_w(y|x)=\frac{1}{Z_w(x)}e^{\sum_{i=1}^nw_if_i(x,y)}
   \\ Z_w(x)=\sum_ye^{\sum_{i=1}^nw_if_i(x,y)}
   $$
   由上式表示的模型$P_w=P_w(y|x)$就是最大熵模型，w是最大熵模型的参数向量，每一维度为对应特征函数的权重。

